{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predicting_Sentence_Probability.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTyvJipXsR/ZL+qHCuOfpr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stephrichter/acceptability-judgements/blob/main/Predicting_Sentence_Probability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapted from https://github.com/jhlau/acceptability-prediction-in-context/blob/master/code/compute_model_score.py"
      ],
      "metadata": {
        "id": "PjgVCIbZbzy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload items from data\n",
        "# upload unigram pickle file"
      ],
      "metadata": {
        "id": "hy5Pyj3M7CXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!gunzip /content/xlnet-large-cased-bookcorpus-wikipedia-openwebtext.pickle.gz"
      ],
      "metadata": {
        "id": "99DdgmYet2Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uBz-nqzUi2i"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODELS = ['bert-large-uncased',\n",
        "          'xlnet-base-cased']\n",
        "\n",
        "####################\n",
        "## argument \"parser\"\n",
        "####################\n",
        "\n",
        "xlnet_pickle = '/content/xlnet-large-cased-bookcorpus-wikipedia-openwebtext.pickle'\n",
        "bert_pickle = '/content/bert-large-uncased-bookcorpus-wikipedia.pickle'\n",
        "\n",
        "class Args:\n",
        "    model_name = MODELS[0]\n",
        "    item_path = '/content/exp3_items_normalized.tsv'\n",
        "    unigram_pickle = bert_pickle if model_name.startswith('bert') else xlnet_pickle\n",
        "\n",
        "    device = 'cuda'\n",
        "    use_context = False # can't be True\n",
        "    xlnet_bidir = True\n",
        "\n",
        "args = Args()"
      ],
      "metadata": {
        "id": "e524-SQYqGWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#df = pd.read_csv(data_path, skiprows=5)\n",
        "df = pd.read_csv(args.item_path, sep='\\t')"
      ],
      "metadata": {
        "id": "RxiG5iLg7gva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Author:         Jey Han Lau\n",
        "Date:           Jul 19\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import argparse\n",
        "import torch\n",
        "import math\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "#from calc_corr import get_sentence_data\n",
        "#from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import BertTokenizer, BertForMaskedLM, XLNetTokenizer, XLNetLMHeadModel \n",
        "from scipy.stats.mstats import pearsonr as corr\n",
        "from scipy.special import softmax\n",
        "\n",
        "#global\n",
        "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
        "(except for Alexei and Maria) are discovered.\n",
        "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
        "remainder of the story. 1883 Western Siberia,\n",
        "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
        "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
        "father initially slaps him for making such an accusation, Rasputin watches as the\n",
        "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
        "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
        "with people, even a bishop, begging for his blessing. <eod> \"\"\"\n",
        "\n",
        "###########\n",
        "#functions#\n",
        "###########\n",
        "def model_score(tokenize_input, tokenize_context, model, tokenizer, device, args):\n",
        "\n",
        "    if args.model_name.startswith(\"gpt\"):\n",
        "\n",
        "        if not args.use_context:\n",
        "\n",
        "            #prepend the sentence with <|endoftext|> token, so that the loss is computed correctly\n",
        "            tensor_input = torch.tensor([[50256] + tokenizer.convert_tokens_to_ids(tokenize_input)], device=device)\n",
        "            labels = torch.tensor([[50256] + tokenizer.convert_tokens_to_ids(tokenize_input)], device=device)\n",
        "            labels[:,:1] = -1\n",
        "            loss = model(tensor_input, labels=tensor_input)\n",
        "\n",
        "        else:\n",
        "            \n",
        "            tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_context+tokenize_input)], device=device)\n",
        "            labels = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_context+tokenize_input)], device=device)\n",
        "            #-1 label for context (loss not computed over these tokens)\n",
        "            labels[:,:len(tokenize_context)] = -1\n",
        "            loss = model(tensor_input, labels=labels)\n",
        "\n",
        "        return float(loss[0]) * -1.0 * len(tokenize_input)\n",
        "\n",
        "    elif args.model_name.startswith(\"bert\"):\n",
        "\n",
        "        batched_indexed_tokens = []\n",
        "        batched_segment_ids = []\n",
        "\n",
        "        if not args.use_context:\n",
        "            tokenize_combined = [\"[CLS]\"] + tokenize_input + [\"[SEP]\"]\n",
        "        else:\n",
        "            tokenize_combined = [\"[CLS]\"] + tokenize_context + tokenize_input + [\"[SEP]\"]\n",
        "\n",
        "        for i in range(len(tokenize_input)):\n",
        "\n",
        "            # Mask a token that we will try to predict back with `BertForMaskedLM`\n",
        "            masked_index = i + 1 + (len(tokenize_context) if args.use_context else 0)\n",
        "            tokenize_masked = tokenize_combined.copy()\n",
        "            tokenize_masked[masked_index] = '[MASK]'\n",
        "            #unidir bert\n",
        "            #for j in range(masked_index, len(tokenize_combined)-1):\n",
        "            #    tokenize_masked[j] = '[MASK]'\n",
        "\n",
        "            # Convert token to vocabulary indices\n",
        "            indexed_tokens = tokenizer.convert_tokens_to_ids(tokenize_masked)\n",
        "            # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
        "            segment_ids = [0]*len(tokenize_masked)\n",
        "\n",
        "            batched_indexed_tokens.append(indexed_tokens)\n",
        "            batched_segment_ids.append(segment_ids)\n",
        "\n",
        "        # Convert inputs to PyTorch tensors\n",
        "        tokens_tensor = torch.tensor(batched_indexed_tokens, device=device)\n",
        "        segment_tensor = torch.tensor(batched_segment_ids, device=device)\n",
        "\n",
        "        # Predict all tokens\n",
        "        with torch.no_grad():\n",
        "            outputs = model(tokens_tensor, token_type_ids=segment_tensor)\n",
        "            predictions = outputs[0]\n",
        "\n",
        "        # go through each word and sum their logprobs\n",
        "        lp = 0.0\n",
        "        for i in range(len(tokenize_input)):\n",
        "            masked_index = i + 1 + (len(tokenize_context) if args.use_context else 0)\n",
        "            predicted_score = predictions[i, masked_index]\n",
        "            predicted_prob = softmax(predicted_score.cpu().numpy())\n",
        "            lp += np.log(predicted_prob[tokenizer.convert_tokens_to_ids([tokenize_combined[masked_index]])[0]])\n",
        "\n",
        "        return lp\n",
        "\n",
        "    elif args.model_name.startswith(\"xlnet\"):\n",
        "\n",
        "        tokenize_ptext = tokenizer.tokenize(PADDING_TEXT.lower())\n",
        "\n",
        "        if not args.use_context:\n",
        "            tokenize_input2 = tokenize_ptext + tokenize_input\n",
        "        else:\n",
        "            tokenize_input2 = tokenize_ptext + tokenize_context + tokenize_input\n",
        "\n",
        "        # go through each word and sum their logprobs\n",
        "        lp = 0.0\n",
        "        for max_word_id in range((len(tokenize_input2)-len(tokenize_input)), (len(tokenize_input2))):\n",
        "\n",
        "            sent = tokenize_input2[:]\n",
        "            input_ids = torch.tensor([tokenizer.convert_tokens_to_ids(sent)], device=device)\n",
        "            perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float, device=device)\n",
        "\n",
        "            #if not bidir, mask target word + right/future words\n",
        "            if not args.xlnet_bidir:\n",
        "                perm_mask[:, :, max_word_id:] = 1.0\n",
        "            #if bidir, mask only the target word\n",
        "            else:\n",
        "                perm_mask[:, :, max_word_id] = 1.0\n",
        "\n",
        "            target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float, device=device)\n",
        "            target_mapping[0, 0, max_word_id] = 1.0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids, perm_mask=perm_mask, target_mapping=target_mapping)\n",
        "                next_token_logits = outputs[0]\n",
        "\n",
        "            word_id = tokenizer.convert_tokens_to_ids([tokenize_input2[max_word_id]])[0]\n",
        "            predicted_prob = softmax((next_token_logits[0][-1]).cpu().numpy())\n",
        "            lp += np.log(predicted_prob[word_id])\n",
        "\n",
        "        return lp"
      ],
      "metadata": {
        "id": "WnSFsuwXp2jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load pre-trained model and tokenizer\n",
        "if args.model_name.startswith(\"gpt\"):\n",
        "    model = GPT2LMHeadModel.from_pretrained(args.model_name)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(args.model_name)\n",
        "elif args.model_name.startswith(\"bert\"):\n",
        "    model = BertForMaskedLM.from_pretrained(args.model_name)\n",
        "    tokenizer = BertTokenizer.from_pretrained(args.model_name,\n",
        "        do_lower_case=(True if \"uncased\" in args.model_name else False))\n",
        "elif args.model_name.startswith(\"xlnet\"):\n",
        "    tokenizer = XLNetTokenizer.from_pretrained(args.model_name)\n",
        "    model = XLNetLMHeadModel.from_pretrained(args.model_name)\n",
        "else:\n",
        "    print(\"Supported models: gpt, bert and xlnet only.\")\n",
        "    raise SystemExit\n",
        "\n",
        "#put model to device (GPU/CPU)\n",
        "device = torch.device(args.device)\n",
        "model.to(device)\n",
        "\n",
        "#eval mode; no dropout\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "Z-LvR8L0d3_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######\n",
        "#main#\n",
        "######\n",
        "#def main(args):\n",
        "\n",
        "#sentence and human ratings\n",
        "#sentencexdata = get_sentence_data(args.input_csv)\n",
        "#human_ratings = pickle.load(open(args.human_rating_pickle, \"rb\"))\n",
        "\n",
        "#unigram frequencies\n",
        "unigram_freq = pickle.load(open(args.unigram_pickle, \"rb\"))\n",
        "unigram_total = sum(unigram_freq.values()) \n",
        "\n",
        "#system scores\n",
        "lps = []\n",
        "mean_lps = []\n",
        "pen_lps = []\n",
        "div_lps = []\n",
        "sub_lps = []\n",
        "slors = []\n",
        "pen_slors = []\n",
        "sent_ids = []\n",
        "\n",
        "#loop through each sentence and compute system scores\n",
        "y = [] #human mean rating\n",
        "sent_total = 0\n",
        "for sent_id, sent in tqdm(enumerate(df['sentence'])):\n",
        "\n",
        "    #y.append(np.mean(ratings))\n",
        "\n",
        "    #text = sentencexdata[sent_id][\"SENTENCE\"]\n",
        "    text = sent\n",
        "    #uppercase first character\n",
        "    #text = text[0].upper() + text[1:]\n",
        "    tokenize_input = tokenizer.tokenize(text)\n",
        "    text_len = len(tokenize_input)\n",
        "\n",
        "    if args.use_context:\n",
        "        context = sentencexdata[sent_id][\"CONTEXT\"].replace(\"\\t\", \" \")\n",
        "        tokenize_context = tokenizer.tokenize(context)\n",
        "    else:\n",
        "        tokenize_context = None\n",
        "\n",
        "    #unigram logprob\n",
        "    uni_lp = 0.0\n",
        "    for w in tokenize_input:\n",
        "        uni_lp += math.log(float(unigram_freq[w])/unigram_total)\n",
        "\n",
        "    #compute sentence logprob\n",
        "    lp = model_score(tokenize_input, tokenize_context, model, tokenizer, device, args)\n",
        "\n",
        "    #acceptability measures\n",
        "    penalty = ((5+text_len)**0.8 / (5+1)**0.8)\n",
        "    lps.append(lp)\n",
        "    mean_lps.append(lp/text_len)\n",
        "    pen_lps.append( lp / penalty )\n",
        "    div_lps.append(-lp / uni_lp)\n",
        "    sub_lps.append(lp - uni_lp)\n",
        "    slors.append((lp - uni_lp) / text_len)\n",
        "    pen_slors.append((lp - uni_lp) / penalty)\n",
        "    sent_ids.append(sent_id)\n",
        "\n",
        "    sent_total += 1"
      ],
      "metadata": {
        "id": "z9ktjiI-q2Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for lp, sent in zip(pen_lps, df['sentence']):\n",
        "  print(lp, sent)"
      ],
      "metadata": {
        "id": "NJ5xWsEkyRKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['penalized_logprob'] = pen_lps"
      ],
      "metadata": {
        "id": "YIR2M71Kce5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('exp3_items_normalized.scored.tsv', sep='\\t', index=False, header=True)"
      ],
      "metadata": {
        "id": "k5Zw3RHwcpkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "CIvXF2s9cpm9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}